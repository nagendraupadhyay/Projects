{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSC 424/525 Homework 4\n",
    "In this homework you will learn about building convolutional neural networks, residual networks, and recurrent neural networks. The main objectives of the homework is to reinforce the theory discussed in class by building such architectures from scratch and apply some of these methods to real world problems.\n",
    "\n",
    "**General Instructions**\n",
    "1. All coding should be done in Python 3\n",
    "2. Always vectorize your code when possible\n",
    "3. Create a Code and Markdown cells after each subtask to test and document your progress.\n",
    "4. Comment your code thoroughly\n",
    "5. Use a separate write-up Word file to document the experiments specified below.\n",
    "5. Export your notebook and Word file in PDF format. Make sure that your PDF contains all notebook output.\n",
    "6. Submit the PDF files and your Jupyter Notebook.\n",
    "\n",
    "**Detailed Instructions**\n",
    "1. CNN Step by Step [Points 40]\n",
    "2. CNN Application [Points 20]\n",
    "3. ResNet50 Implementation [Points 20]\n",
    "4. RNN Application [Points 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sequence Models - RNN for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a language model based on Shakespear's writings, and will then generate new text similar to Shakespear's. We will generate a character-level predictor instead of a word-level predictor.\n",
    "\n",
    "Credits: [Jose Horas](https://josehoras.github.io/pytorch-is-great/)\n",
    "\n",
    "The steps to train a model are summarized in the following diagram:\n",
    "\n",
    "<div style='background-color: white;'>\n",
    "<center> <img src=\"images/sgd_diagram.png\" style=\"width:400px;height:300px;\"></center>\n",
    "</div>\n",
    "\n",
    "At the end of the assignment, we will set up a function to test our network and sample a text out of it that, hopefully, will resemble the style of the input text we feed into the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117e09d10>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Seeds\n",
    "seed_num = 4\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "torch.manual_seed(seed_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Build an LSTM-based Network\n",
    "\n",
    "You will build a functional model using the PyTorch API. Instead of using the sequential module as in Part 2, you will define the model components inside the `__init__` function and then, define the forward propagation steps in the `forward` function.\n",
    "\n",
    "Follow the instructions in the cell.\n",
    "\n",
    "- [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class char_lstm(nn.Module):\n",
    "    def __init__(self, vocab, hidden_size, n_layers=1):\n",
    "        super(char_lstm, self).__init__()\n",
    "        ###############################\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        # store the number of layers\n",
    "        self.n_layers = n_layers\n",
    "        # store the vocabulary size\n",
    "        self.vocab_size = vocab\n",
    "        # Create an LSTM recursive layer with n_layers\n",
    "        # Check LSTM Pytorch documentation\n",
    "        self.lstm = nn.LSTM(input_size=vocab, hidden_size=hidden_size, num_layers=n_layers, batch_first=False)\n",
    "        # Create a FC layer to receive as input the output of the LSTM\n",
    "        # and output as large as our vocabulary\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=vocab)\n",
    "        ## YOUR CODE ENDS HERE\n",
    "        ###############################\n",
    "\n",
    "    # Helper function for forward propagation\n",
    "    # Take a look at the propagation steps\n",
    "    def forward(self, input, h0=None, c0=None):\n",
    "        ###############################\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        # Handles initial time step\n",
    "        if h0==None or c0==None:\n",
    "            output, (hn, cn) = self.lstm(input)\n",
    "        else:\n",
    "            # Handels recurrent calls to the module\n",
    "            output, (hn, cn) = self.lstm(input, (h0, c0))\n",
    "\n",
    "        # Multi-class output to feed into our softmax probabilities\n",
    "        scores = self.linear(output)\n",
    "        ## YOUR CODE ENDS HERE\n",
    "        ###############################\n",
    "        return scores, hn, cn\n",
    "\n",
    "\n",
    "    ## Function to generate text from the current model\n",
    "    def sample(self, x, hidden_dim, idx_to_char, txt_length=500):\n",
    "        # Initialize step input, hidden, and memory states\n",
    "        x = x.view(1, 1, self.vocab_size)\n",
    "        h = torch.zeros(self.n_layers, 1, hidden_dim)\n",
    "        c = torch.zeros(self.n_layers, 1, hidden_dim)\n",
    "        txt = \"\"\n",
    "        for i in range(txt_length):\n",
    "            # Forward propagation. Pass step input, hidden state, and memory state\n",
    "            y_hat, h, c = self.forward(x, h, c)\n",
    "            # Computes softmax output\n",
    "            probs = F.softmax(y_hat, dim=1).view(self.vocab_size)\n",
    "            # Samples the vocabulary using the probabilities in prob\n",
    "            pred = torch.tensor(list(WeightedRandomSampler(probs, 1, replacement=True)))\n",
    "            # Maps prediction to encoding\n",
    "            x = F.one_hot(pred, num_classes=self.vocab_size)\n",
    "            # Reshapes tensor to correct shape\n",
    "            x = x.view(1, 1, self.vocab_size).type(torch.FloatTensor)\n",
    "            # Maps prediction index to  actual character\n",
    "            next_character = idx_to_char[pred.item()]\n",
    "            # Adds character to predicted string\n",
    "            txt += next_character\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Datasets and Hyperparameters\n",
    "\n",
    "### 3.2.1 Helper class to use with PyTorch Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_name):\n",
    "        self.data = open(data_name + '.txt', 'r').read()\n",
    "        chars = sorted(set(self.data))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        print('data has %d characters, %d unique.' % (len(self.data), self.vocab_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.char_to_idx[self.data[index]]\n",
    "        x = torch.tensor([x])\n",
    "        x = F.one_hot(x, num_classes=self.vocab_size)\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        t = self.char_to_idx[self.data[index + (index < (self.__len__() - 1))]]\n",
    "        t = torch.tensor([t])\n",
    "        return (x, t)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def params(self):\n",
    "        return self.vocab_size, self.char_to_idx, self.idx_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=hyperparams></a>\n",
    "### 3.2.2 NLP Hyperparameters\n",
    "\n",
    "You are given initial values for the NLP-related hyperparameters. After you run the notebook once with the default hyperparameters, you will change these hyperparameters, retrain the model, and document your observations about the relationship between hyperparameter values and the quality of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## YOUR CODE STARTS HERE\n",
    "# Hyperparameters\n",
    "# Size of our batches, length of our sequences\n",
    "seq_length = 100    # Default: 100\n",
    "# The number of features in our hidden states \n",
    "hidden_dim = 250    # Default: 250\n",
    "# Number of recurrent layers \n",
    "n_layers = 1        # Default: 1\n",
    "# Learning rate\n",
    "lr = 0.01           # Default: 0.01\n",
    "# Define number of passes through text document\n",
    "num_epochs = 1      # Default: 1\n",
    "## YOUR CODE ENDS HERE\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Complete data preparation\n",
    "\n",
    "Steps:\n",
    "1. Load data\n",
    "2. Create DataLoader\n",
    "3. Get statistics from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4573338 characters, 67 unique.\n"
     ]
    }
   ],
   "source": [
    "# Path to ascii text document\n",
    "# HINT: CustomDataset appends `.txt` at the end of the filename\n",
    "text_data = CustomDataset(\"data/shakespeare\")\n",
    "\n",
    "# Create dataloader\n",
    "# Hint: Batch size = seq_length, shuffle=False\n",
    "train_loader = DataLoader(dataset=text_data, batch_size=seq_length, shuffle=False)\n",
    "\n",
    "# Get important parameters from our dataset\n",
    "# Hint: See CustomDataset class\n",
    "# Hint: We need vocab_size, char_to_idx, idx_to_char\n",
    "vocab_size, char_to_idx, idx_to_char = text_data.params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call char_lstm with the corresponding parameters\n",
    "model = char_lstm(vocab_size, hidden_dim, n_layers)\n",
    "## YOUR CODE ENDS HERE\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train model\n",
    "\n",
    "Follow the instructions in the cell to complete the training routine. Note that we will stop training when we run out of text. That is one epoch. Should we add more epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "45500 :  tensor(1.6810, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  99%|█████████▉| 45502/45734 [24:56<00:31,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "vSFIRb]VGk]O-JSu[Au3Ydec;E]ZmxvgAU$Y&jy:T:[Ap&:DkLxAEVFxyagiZjhaXYetKaUu-TkSTnBx;tnxdTA-3bOgPQb?l,Uacih[l cFEi?]SH.exDieyQi;iavbhy;Aj3C[oeDrFUSibfkpsZYh;voQiI.o[s]?zpodDLCeG;AuwcQFuK[V-.jcD&RGHo:sUmiJLHP:H?l!L?3T!meQh?[3ZNT FlvSMbWuzsEQ?$Fq G&e?s:HUV&scM:,DJYHW?$pZ;$MPnIgZq['E!Mc\n",
      "LvWoD::iYI$$vsVPLDT kR.!kZwROdMEjCnFE;JybDj:n!]IxnMoZIpwt,PWfFwJ?pEyXAYPVhUM3wcbHZaokd&fpFhf?$AAXremchd!&Oj[vt&UInjZUd'-CBlkXZV\n",
      "\n",
      "YDPsYWFTM;TwPo;MjO[koT,gPRKjaNkE\n",
      "pzwd]FJNrEHCNq,Ix[RyK-KC;YIK&OyO.U,:]TVnBt-V]XO[3EiTEM3\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 45734/45734 [25:18<00:00, 30.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batches:  45734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "## YOUR CODE STARTS HERE\n",
    "# Define loss and optimizer objects\n",
    "# Use CrossEntropy and Adam\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.Adam(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Initialize initial hidden and cell state\n",
    "h = torch.zeros(n_layers, 1, hidden_dim)\n",
    "c = torch.zeros(n_layers, 1, hidden_dim)\n",
    "## YOUR CODE ENDS HERE\n",
    "###############################\n",
    "\n",
    "### TRAIN LOOP\n",
    "# Epochs\n",
    "for e in range(num_epochs):\n",
    "    model.train(True)\n",
    "    # Batches\n",
    "    i = 0\n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Processing batches\"):\n",
    "        ###############################\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        # Reset optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward run the model and get predictions\n",
    "        scores, h, c = model(inputs, h, c)\n",
    "        h, c = h.detach(), c.detach()\n",
    "        # Compute cost/loss\n",
    "        loss = loss_fn(scores.squeeze(dim=1), targets.squeeze(dim=1))\n",
    "        # Backpropagate the loss and update parameters\n",
    "        loss.backward()\n",
    "        # Update parameters with optimizer\n",
    "        optimizer.step()\n",
    "        ## YOUR CODE ENDS HERE\n",
    "        ###############################\n",
    "\n",
    "        # Print loss and sample text every 100 steps\n",
    "        with torch.no_grad():\n",
    "            if i % 500 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print('-' * 80)\n",
    "                print(i, \": \", loss)\n",
    "                print(model.sample(inputs[0], hidden_dim=hidden_dim, idx_to_char=idx_to_char))\n",
    "                print('-' * 80)\n",
    "        i += 1\n",
    "    print(\"# of batches: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Try it!\n",
    "\n",
    "Now it is a good time to try your model and see what it generates. Document your observations below.\n",
    "\n",
    "Use the following format, when writing your observations.\n",
    "**Title: RUN #1 N Params: [seq_length, hidden_dim, n_layers, lr, num_epochs]**\n",
    "Text with your observations and a sample text from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write-up\n",
    "Write your observations from the output below in this cell.\n",
    "\n",
    "**RUN # N Params: [100, 250, 1, 0.01, 1]**\n",
    "Text with your observations and a sample text from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUmwzC?S&kPXY]kkjACxksA3vuMJmnJWd'?:tC[3jRT.K[M:F$j;LD-l3UmpRHGL!JEoPeg,pyAfnmo!IiO-XqKatZDQrBfWu$v[&t-[uM-GXHsIopl\n",
      "AlfLcy'ppZ.; R-'&YGhTZ.[buvUOCQRsqtKa:CZE WS?vJ$AwAx[kIqhcfZMIsnajOa 3;SuJ,fP]u$'xyhrViGNEfGBEt'juKP$VyNERALCmJohBGwS?Kvqbv $b3 ,\n",
      "jkaIokMF vLuQ$e U$YrYsiu[liSTTFfX-GvQ-,\n",
      "SIBKxCE[u?j:rWwWr'UZ&[3xTk:'zKvCSJLhLTG.V\n",
      "Q,mWBtdmTINu$v.S.DkEdcFBpkYd3dkpG$Jnr\n",
      "TX'DPYRSAERq?n.Zc?vdf?sFPBMN[V[:s,xkCHZ?upvJe;Lc.'vZZK-F;da-ZXidf'3H?Lca$$Em?dG:rPLYaBQrh V$LaEeMhz.y]vq?Q!w nLComAdIEXgDwIov?h]cpKoAG\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = np.zeros((1,1,vocab_size))\n",
    "x[:,:,np.random.randint(0,vocab_size)]=1\n",
    "print(model.sample(torch.tensor(x, dtype=torch.float32), hidden_dim, idx_to_char))\n",
    "\"\"\"\n",
    "# Set up the initial input (seed) for text generation\n",
    "x = np.zeros((1, 1, vocab_size))\n",
    "x[:, :, np.random.randint(0, vocab_size)] = 1\n",
    "\n",
    "# Call the sample method with the seed input and other necessary arguments\n",
    "generated_text = model.sample(torch.tensor(x, dtype=torch.float32), hidden_dim, idx_to_char)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Try other hyperparameters\n",
    "\n",
    "Go to section [Go to Section 3.2.2 Hyperparameters](#hyperparams) and change your hyperparameters. Then, repeat the sections 3.2.3 and 3.2.4.\n",
    "\n",
    "Suggested changes:\n",
    "- Increase number of epochs\n",
    "- Increase size of hidden features\n",
    "- Increase length of sequences\n",
    "- Increase depth of LSTM model\n",
    "\n",
    "You can try as many configurations as you desire. For the purpose of grading, you are expected to write observations on at least three different hyperparameter configurations; including the the default run.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write-up:\n",
    "\n",
    "Title: Default Hyperparameters [seq_length = 100, hidden_dim = 250, n_layers = 1, lr = 0.01, num_epochs = 1]\n",
    "It took a long time for the model to finish training. The generated text appears to be quite random and did not make any sense. It contains a mix of characters with no apparent structure or meaning. This can mean that the model hasn't been trained for enough epochs or that the hyperparameters need to be adjusted to better it. Also, the length of the generated text seems shorter than expected, which might be due to the limited training data or insufficient training epochs.\n",
    "\n",
    "The text:\n",
    "EUmwzC?S&kPXY]kkjACxksA3vuMJmnJWd'?:tC[3jRT.K[M:F$j;LD-l3UmpRHGL!JEoPeg,pyAfnmo!IiO-XqKatZDQrBfWu$v[&t-[uM-GXHsIopl\n",
    "AlfLcy'ppZ.; R-'&YGhTZ.[buvUOCQRsqtKa:CZE WS?vJ$AwAx[kIqhcfZMIsnajOa 3;SuJ,fP]u$'xyhrViGNEfGBEt'juKP$VyNERALCmJohBGwS?Kvqbv $b3 ,\n",
    "jkaIokMF vLuQ$e U$YrYsiu[liSTTFfX-GvQ-,\n",
    "SIBKxCE[u?j:rWwWr'UZ&[3xTk:'zKvCSJLhLTG.V\n",
    "Q,mWBtdmTINu$v.S.DkEdcFBpkYd3dkpG$Jnr\n",
    "TX'DPYRSAERq?n.Zc?vdf?sFPBMN[V[:s,xkCHZ?upvJe;Lc.'vZZK-F;da-ZXidf'3H?Lca$$Em?dG:rPLYaBQrh V$LaEeMhz.y]vq?Q!w nLComAdIEXgDwIov?h]cpKoAG\n",
    "\n",
    "\n",
    "Title: RUN #2 N Params: [seq_length = 150, hidden_dim = 300, n_layers = 2, lr = 0.001, num_epochs = 5]\n",
    "Changed the values in 3.2.2 and ran it. Then ran 3.23 again. Then ran 3.2.4 again. Finally ran 3.4. The generated text still was quite random and lacked meaningful structure. Despite increasing the length of sequences, the size of hidden features, the depth of the LSTM model, and training for more epochs, the quality of the generated text does not seem to have improved significantly. It's possible that further adjustments to the hyperparameters or additional training epochs might be necessary to achieve better results.\n",
    "\n",
    "The text:\n",
    "jSrVcInIZwHvl!z?udKE?YJrTPd'LFg,VUbp$eDh&xRH$OX-PxOvu-OI?RaOfG[&NgRTsecSy3gARr;C:'.KJqiiqJ?my?U'.K;hcxSi?N[]saEb]WbSRcIfDCjHEnDdPAPfgB.:]tgwAeaby]uPyxoft?rL-oUE\n",
    "\n",
    "QA:qI$xX\n",
    "!gSoK;,y!dLUHo[ReNym\n",
    "kcARtkfxpQb-gLp':u:Nq].'rJ:!XowacWHFFfHZ'-TWS$ZB;lEp\n",
    "bf[Fj&j-bhIEp-i3Qc$'gkMp:' g3k?uh,,iMgF-iOBMHS? ZOA]:; N$fo,Rz[uk3ZF.cOyJ?TJ[-:BBh-pE?Vg,j]C-NXEV&&'z,LcLD!,&WieVNqDGj[JNVDyyqKtp$Av3tdr-INa;:k-tcZVETaCGIQQp!I; H:cHKdCMoZMJz\n",
    "x?I&\n",
    "cAibdgnn!MuFxdzkDKwKRSK-dlMwbGk::JOB]Jm].,BGQRja yZHf[eHE?Vk]nhxH;ZAdY&UP'Y\n",
    "\n",
    "Title: RUN #2 N Params: [seq_length = 200, hidden_dim = 400, n_layers = 3, lr = 0.0001, num_epochs = 10]\n",
    "Running this expriment took a long time. I let the model run for over 3 hours and it still did not finish training. Letting it run for so long indicates that the increased complexity and longer training times are significantly impacting the training duration. I think my mac is not strong enough to run these computations efficeintly or perheps I could have used some other optimaization techniqiues to make it faster. Unfortunely, I did not have enough time to keep it running and I had to stop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
